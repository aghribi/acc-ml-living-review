{
  "papers": {
    "('2506.03796', '', 'geoff the generic optimization framework frontend for particle accelerator controls')": {
      "id": "arxiv:2506.03796",
      "doi": null,
      "arxiv_id": "2506.03796",
      "inspire_id": null,
      "title": "Geoff: The Generic Optimization Framework & Frontend for Particle Accelerator Controls",
      "authors": [
        "Penelope Madysa",
        "Sabrina Appel",
        "Verena Kain",
        "Michael Schenk"
      ],
      "abstract": "Geoff is a collection of Python packages that form a framework for automation\nof particle accelerator controls. With particle accelerator laboratories around\nthe world researching machine learning techniques to improve accelerator\nperformance and uptime, a multitude of approaches and algorithms have emerged.\nThe purpose of Geoff is to harmonize these approaches and to minimize friction\nwhen comparing or migrating between them. It provides standardized interfaces\nfor optimization problems, utility functions to speed up development, and a\nreference GUI application that ties everything together. Geoff is an\nopen-source library developed at CERN and maintained and updated in\ncollaboration between CERN and GSI as part of the EURO-LABS project. This paper\ngives an overview over Geoff's design, features, and current usage.",
      "date": "2025-09-16",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Tools & Libraries",
          "score": 1.0
        },
        {
          "label": "Novel Applications",
          "score": 0.5761764049530029
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2506.03796v3"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:32:23Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:32:23Z"
    },
    "('2409.06336', '', 'towards agentic ai on particle accelerators')": {
      "id": "arxiv:2409.06336",
      "doi": null,
      "arxiv_id": "2409.06336",
      "inspire_id": null,
      "title": "Towards Agentic AI on Particle Accelerators",
      "authors": [
        "Antonin Sulc",
        "Thorsten Hellert",
        "Raimund Kammering",
        "Hayden Hoschouer",
        "Jason St. John"
      ],
      "abstract": "As particle accelerators grow in complexity, traditional control methods face\nincreasing challenges in achieving optimal performance. This paper envisions a\nparadigm shift: a decentralized multi-agent framework for accelerator control,\npowered by Large Language Models (LLMs) and distributed among autonomous\nagents. We present a proposition of a self-improving decentralized system where\nintelligent agents handle high-level tasks and communication and each agent is\nspecialized to control individual accelerator components.\n  This approach raises some questions: What are the future applications of AI\nin particle accelerators? How can we implement an autonomous complex system\nsuch as a particle accelerator where agents gradually improve through\nexperience and human feedback? What are the implications of integrating a\nhuman-in-the-loop component for labeling operational data and providing expert\nguidance? We show three examples, where we demonstrate the viability of such\narchitecture.",
      "date": "2025-09-02",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Reinforcement Learning & Autonomous Systems",
          "score": 0.5981404781341553
        },
        {
          "label": "Novel Applications",
          "score": 0.589080810546875
        },
        {
          "label": "Tools & Libraries",
          "score": 1.0
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2409.06336v4"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:32:23Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:32:23Z"
    },
    "('2509.12030', '', 'acceleration of multi scale lts magnet simulations with neural network surrogate models')": {
      "id": "arxiv:2509.12030",
      "doi": null,
      "arxiv_id": "2509.12030",
      "inspire_id": null,
      "title": "Acceleration of Multi-Scale LTS Magnet Simulations with Neural Network Surrogate Models",
      "authors": [
        "Louis Denis",
        "Julien Dular",
        "Vincent Nuttens",
        "Mariusz Wozniak",
        "Beno√Æt Vanderheyden",
        "Christophe Geuzaine"
      ],
      "abstract": "While the prediction of AC losses during transients is critical for designing\nlarge-scale low-temperature superconducting (LTS) magnets, brute-force\nfinite-element (FE) simulation of their detailed geometry down to the length\nscale of the conductors is a computational challenge. Multi-scale methods,\nbalancing between a coarse approximation of the fields at the scale of the\nmagnet and a detailed description at the scale of the conductors, are promising\napproaches to reduce the computational load while keeping a sufficient\naccuracy. In this work, we introduce a neural network approach to accelerate\nmulti-scale magneto-thermal simulations of LTS magnets by replacing costly\nsingle-turn FE models with neural network surrogates. The neural network\narchitecture is presented and discussed, together with an automated procedure\nfor generating simulation data for its training. The resulting accelerated\nmulti-scale model is used to simulate current ramp-up procedures for the IBA\nS2C2 magnet. The surrogate-based multi-scale model is compared with a\nconventional multi-scale model based on a composite wire-in-channel FE model.\nThe surrogate model is shown to reproduce single-turn filament hysteresis,\ninter-filament coupling, and eddy losses, while the computational time of the\nmulti-scale method is reduced by a factor of 800.",
      "date": "2025-09-15",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Surrogate Models",
          "score": 1.0
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.12030v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:32:29Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:32:29Z"
    },
    "('2509.02227', '', 'application of large language models for the extraction of information from particle accelerator technical documentation')": {
      "id": "arxiv:2509.02227",
      "doi": null,
      "arxiv_id": "2509.02227",
      "inspire_id": null,
      "title": "Application Of Large Language Models For The Extraction Of Information From Particle Accelerator Technical Documentation",
      "authors": [
        "Qing Dai",
        "Rasmus Ischebeck",
        "Maruisz Sapinski",
        "Adam Grycner"
      ],
      "abstract": "The large set of technical documentation of legacy accelerator systems,\ncoupled with the retirement of experienced personnel, underscores the urgent\nneed for efficient methods to preserve and transfer specialized knowledge. This\npaper explores the application of large language models (LLMs), to automate and\nenhance the extraction of information from particle accelerator technical\ndocuments. By exploiting LLMs, we aim to address the challenges of knowledge\nretention, enabling the retrieval of domain expertise embedded in legacy\ndocumentation. We present initial results of adapting LLMs to this specialized\ndomain. Our evaluation demonstrates the effectiveness of LLMs in extracting,\nsummarizing, and organizing knowledge, significantly reducing the risk of\nlosing valuable insights as personnel retire. Furthermore, we discuss the\nlimitations of current LLMs, such as interpretability and handling of rare\ndomain-specific terms, and propose strategies for improvement. This work\nhighlights the potential of LLMs to play a pivotal role in preserving\ninstitutional knowledge and ensuring continuity in highly specialized fields.",
      "date": "2025-09-02",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Statistics & Trends",
          "score": 0.39795249700546265
        },
        {
          "label": "Tools & Libraries",
          "score": 0.38388609886169434
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.02227v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:33:20Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:33:20Z"
    },
    "('2509.26137', '', 'accelerating transformers in online rl')": {
      "id": "arxiv:2509.26137",
      "doi": null,
      "arxiv_id": "2509.26137",
      "inspire_id": null,
      "title": "Accelerating Transformers in Online RL",
      "authors": [
        "Daniil Zelezetsky",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "The appearance of transformer-based models in Reinforcement Learning (RL) has\nexpanded the horizons of possibilities in robotics tasks, but it has\nsimultaneously brought a wide range of challenges during its implementation,\nespecially in model-free online RL. Some of the existing learning algorithms\ncannot be easily implemented with transformer-based models due to the\ninstability of the latter. In this paper, we propose a method that uses the\nAccelerator policy as a transformer's trainer. The Accelerator, a simpler and\nmore stable model, interacts with the environment independently while\nsimultaneously training the transformer through behavior cloning during the\nfirst stage of the proposed algorithm. In the second stage, the pretrained\ntransformer starts to interact with the environment in a fully online setting.\nAs a result, this model-free algorithm accelerates the transformer in terms of\nits performance and helps it to train online in a more stable and faster way.\nBy conducting experiments on both state-based and image-based ManiSkill\nenvironments, as well as on MuJoCo tasks in MDP and POMDP settings, we show\nthat applying our algorithm not only enables stable training of transformers\nbut also reduces training time on image-based environments by up to a factor of\ntwo. Moreover, it decreases the required replay buffer size in off-policy\nmethods to 10-20 thousand, which significantly lowers the overall computational\ndemands.",
      "date": "2025-09-30",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Reinforcement Learning & Autonomous Systems",
          "score": 0.4919734597206116
        },
        {
          "label": "Tools & Libraries",
          "score": 0.3567287027835846
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.26137v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:33:30Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:33:30Z"
    },
    "('2509.25104', '', 'towards generalizable deep ptychography neural networks')": {
      "id": "arxiv:2509.25104",
      "doi": null,
      "arxiv_id": "2509.25104",
      "inspire_id": null,
      "title": "Towards generalizable deep ptychography neural networks",
      "authors": [
        "Albert Vong",
        "Steven Henke",
        "Oliver Hoidn",
        "Hanna Ruth",
        "Junjing Deng",
        "Alexander Hexemer",
        "Apurva Mehta",
        "Arianna Gleason",
        "Levi Hancock",
        "Nicholas Schwarz"
      ],
      "abstract": "X-ray ptychography is a data-intensive imaging technique expected to become\nubiquitous at next-generation light sources delivering many-fold increases in\ncoherent flux. The need for real-time feedback under accelerated acquisition\nrates motivates surrogate reconstruction models like deep neural networks,\nwhich offer orders-of-magnitude speedup over conventional methods. However,\nexisting deep learning approaches lack robustness across diverse experimental\nconditions. We propose an unsupervised training workflow emphasizing probe\nlearning by combining experimentally-measured probes with synthetic,\nprocedurally generated objects. This probe-centric approach enables a single\nphysics-informed neural network to reconstruct unseen experiments across\nmultiple beamlines; among the first demonstrations of multi-probe\ngeneralization. We find probe learning is equally important as in-distribution\nlearning; models trained using this synthetic workflow achieve reconstruction\nfidelity comparable to those trained exclusively on experimental data, even\nwhen changing the type of synthetic training object. The proposed approach\nenables training of experiment-steering models that provide real-time feedback\nunder dynamic experimental conditions.",
      "date": "2025-09-29",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Others",
          "score": 0.0
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.25104v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:33:30Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:33:30Z"
    },
    "('2509.26411', '', 'trackformers part 2 enhanced transformer based models for high energy physics track reconstruction')": {
      "id": "arxiv:2509.26411",
      "doi": null,
      "arxiv_id": "2509.26411",
      "inspire_id": null,
      "title": "TrackFormers Part 2: Enhanced Transformer-Based Models for High-Energy Physics Track Reconstruction",
      "authors": [
        "Sascha Caron",
        "Nadezhda Dobreva",
        "Maarten Kimpel",
        "Uraz Odyurt",
        "Slav Pshenov",
        "Roberto Ruiz de Austri Bazan",
        "Eugene Shalugin",
        "Zef Wolffs",
        "Yue Zhao"
      ],
      "abstract": "High-Energy Physics experiments are rapidly escalating in generated data\nvolume, a trend that will intensify with the upcoming High-Luminosity LHC\nupgrade. This surge in data necessitates critical revisions across the data\nprocessing pipeline, with particle track reconstruction being a prime candidate\nfor improvement. In our previous work, we introduced \"TrackFormers\", a\ncollection of Transformer-based one-shot encoder-only models that effectively\nassociate hits with expected tracks. In this study, we extend our earlier\nefforts by incorporating loss functions that account for inter-hit\ncorrelations, conducting detailed investigations into (various) Transformer\nattention mechanisms, and a study on the reconstruction of higher-level\nobjects. Furthermore we discuss new datasets that allow the training on hit\nlevel for a range of physics processes. These developments collectively aim to\nboost both the accuracy, and potentially the efficiency of our tracking models,\noffering a robust solution to meet the demands of next-generation high-energy\nphysics experiments.",
      "date": "2025-09-30",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Data Management",
          "score": 0.42183810472488403
        },
        {
          "label": "Novel Applications",
          "score": 0.37437400221824646
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.26411v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:34:10Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:34:10Z"
    },
    "('2509.12945', '', 'fusionmae large scale pretrained model to optimize and simplify diagnostic and control of fusion plasma')": {
      "id": "arxiv:2509.12945",
      "doi": null,
      "arxiv_id": "2509.12945",
      "inspire_id": null,
      "title": "FusionMAE: large-scale pretrained model to optimize and simplify diagnostic and control of fusion plasma",
      "authors": [
        "Zongyu Yang",
        "Zhenghao Yang",
        "Wenjing Tian",
        "Jiyuan Li",
        "Xiang Sun",
        "Guohui Zheng",
        "Songfen Liu",
        "Niannian Wu",
        "Rongpeng Li",
        "Zhaohe Xu",
        "Bo Li",
        "Zhongbing Shi",
        "Zhe Gao",
        "Wei Chen",
        "Xiaoquan Ji",
        "Min Xu",
        "Wulyu Zhong"
      ],
      "abstract": "In magnetically confined fusion device, the complex, multiscale, and\nnonlinear dynamics of plasmas necessitate the integration of extensive\ndiagnostic systems to effectively monitor and control plasma behaviour. The\ncomplexity and uncertainty arising from these extensive systems and their\ntangled interrelations has long posed a significant obstacle to the\nacceleration of fusion energy development. In this work, a large-scale model,\nfusion masked auto-encoder (FusionMAE) is pre-trained to compress the\ninformation from 88 diagnostic signals into a concrete embedding, to provide a\nunified interface between diagnostic systems and control actuators. Two\nmechanisms are proposed to ensure a meaningful embedding: compression-reduction\nand missing-signal reconstruction. Upon completion of pre-training, the model\nacquires the capability for 'virtual backup diagnosis', enabling the inference\nof missing diagnostic data with 96.7% reliability. Furthermore, the model\ndemonstrates three emergent capabilities: automatic data analysis, universal\ncontrol-diagnosis interface, and enhancement of control performance on multiple\ntasks. This work pioneers large-scale AI model integration in fusion energy,\ndemonstrating how pre-trained embeddings can simplify the system interface,\nreducing necessary diagnostic systems and optimize operation performance for\nfuture fusion reactors.",
      "date": "2025-09-16",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Novel Applications",
          "score": 0.3564392626285553
        },
        {
          "label": "Data Management",
          "score": 0.31196922063827515
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.12945v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:34:40Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:34:40Z"
    },
    "('2509.06067', '', 'a surrogate model for high temperature superconducting magnets to predict current distribution with neural network')": {
      "id": "arxiv:2509.06067",
      "doi": null,
      "arxiv_id": "2509.06067",
      "inspire_id": null,
      "title": "A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network",
      "authors": [
        "Mianjun Xiao",
        "Peng Song",
        "Yulong Liu",
        "Cedric Korte",
        "Ziyang Xu",
        "Jiale Gao",
        "Jiaqi Lu",
        "Haoyang Nie",
        "Qiantong Deng",
        "Timing Qu"
      ],
      "abstract": "Finite element method (FEM) is widely used in high-temperature\nsuperconducting (HTS) magnets, but its computational cost increases with magnet\nsize and becomes time-consuming for meter-scale magnets, especially when\nmulti-physics couplings are considered, which limits the fast design of\nlarge-scale REBCO magnet systems. In this work, a surrogate model based on a\nfully connected residual neural network (FCRN) is developed to predict the\nspace-time current density distribution in REBCO solenoids. Training datasets\nwere generated from FEM simulations with varying numbers of turns and pancakes.\nThe results demonstrate that, for deeper networks, the FCRN architecture\nachieves better convergence than conventional fully connected network (FCN),\nwith the configuration of 12 residual blocks and 256 neurons per layer\nproviding the most favorable balance between training accuracy and\ngeneralization capability. Extrapolation studies show that the model can\nreliably predict magnetization losses for up to 50% beyond the training range,\nwith maximum errors below 10%. The surrogate model achieves predictions several\norders of magnitude faster than FEM and still remains advantageous when\ntraining costs are included. These results indicate that the proposed\nFCRN-based surrogate model provides both accuracy and efficiency, offering a\npromising tool for the rapid analysis of large-scale HTS magnets.",
      "date": "2025-09-07",
      "year": 2025,
      "venue": "arXiv",
      "status": "preprint",
      "categories": [
        {
          "label": "Novel Applications",
          "score": 0.30026954412460327
        },
        {
          "label": "Surrogate Models",
          "score": 1.0
        },
        {
          "label": "Tools & Libraries",
          "score": 1.0
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "arxiv": "http://arxiv.org/abs/2509.06067v1"
      },
      "sources": [
        {
          "source": "arxiv",
          "seen_at": "2025-10-01T10:34:40Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:34:40Z"
    },
    "('', '', 'trackformers part 2 enhanced transformer based models for high energy physics track reconstruction')": {
      "id": "hash:3ec2ffa2112d",
      "doi": null,
      "arxiv_id": null,
      "inspire_id": null,
      "title": "TrackFormers Part 2: Enhanced Transformer-Based Models for High-Energy Physics Track Reconstruction",
      "authors": [
        "Caron, Sascha",
        "Dobreva, Nadezhda",
        "Kimpel, Maarten",
        "Odyurt, Uraz",
        "Pshenov, Slav",
        "Bazan, Roberto Ruiz de Austri",
        "Shalugin, Eugene",
        "Wolffs, Zef",
        "Zhao, Yue"
      ],
      "abstract": "High-Energy Physics experiments are rapidly escalating in generated data volume, a trend that will intensify with the upcoming High-Luminosity LHC upgrade. This surge in data necessitates critical revisions across the data processing pipeline, with particle track reconstruction being a prime candidate for improvement. In our previous work, we introduced \"TrackFormers\", a collection of Transformer-based one-shot encoder-only models that effectively associate hits with expected tracks. In this study, we extend our earlier efforts by incorporating loss functions that account for inter-hit correlations, conducting detailed investigations into (various) Transformer attention mechanisms, and a study on the reconstruction of higher-level objects. Furthermore we discuss new datasets that allow the training on hit level for a range of physics processes. These developments collectively aim to boost both the accuracy, and potentially the efficiency of our tracking models, offering a robust solution to meet the demands of next-generation high-energy physics experiments.",
      "date": "2025-09-30",
      "year": 2025,
      "venue": "InspireHEP",
      "status": "published",
      "categories": [
        {
          "label": "Data Management",
          "score": 0.42183810472488403
        },
        {
          "label": "Novel Applications",
          "score": 0.37437400221824646
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "inspire": "https://inspirehep.net/literature/2983540"
      },
      "sources": [
        {
          "source": "inspire",
          "seen_at": "2025-10-01T10:35:38Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:35:38Z"
    },
    "('', '', 'fast accurate and precise detector simulation with vision transformers')": {
      "id": "hash:d347768e3a94",
      "doi": null,
      "arxiv_id": null,
      "inspire_id": null,
      "title": "Fast, accurate, and precise detector simulation with vision transformers",
      "authors": [
        "Favaro, Luigi",
        "Giammanco, Andrea",
        "Krause, Claudius"
      ],
      "abstract": "The speed and fidelity of detector simulations in particle physics pose compelling questions about LHC analysis and future colliders. The sparse high-dimensional data, combined with the required precision, provide a challenging task for modern generative networks. We present a comparison between solutions with different trade-offs, including accurate Conditional Flow Matching and faster coupling-based Normalising Flows. Vision Transformers allows us to emulate the energy deposition from detailed Geant4 simulations. We evaluate the networks using high-level observables, neural network classifiers, and sampling timings, showing minimum deviations from Geant4 while achieving faster generation. We use the CaloChallenge benchmark datasets for reproducibility and further development.",
      "date": "2025-09-29",
      "year": 2025,
      "venue": "InspireHEP",
      "status": "published",
      "categories": [
        {
          "label": "Tools & Libraries",
          "score": 0.40464967489242554
        },
        {
          "label": "Data Management",
          "score": 0.38319894671440125
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "inspire": "https://inspirehep.net/literature/2974326"
      },
      "sources": [
        {
          "source": "inspire",
          "seen_at": "2025-10-01T10:35:42Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:35:42Z"
    },
    "('', '10.3389/fmed.2025.1577034', 'the promise of artificial intelligence assisted radiotherapy for prostate cancer in morocco a transformational opportunity')": {
      "id": "doi:10.3389/fmed.2025.1577034",
      "doi": "10.3389/fmed.2025.1577034",
      "arxiv_id": null,
      "inspire_id": null,
      "title": "The promise of artificial intelligence-assisted radiotherapy for prostate cancer in Morocco: a transformational opportunity",
      "authors": [
        "Fadila Kouhen",
        "Meryem Naciri",
        "Hanae El Gouache",
        "Nadia Errafiy",
        "Abdelhak Maghous"
      ],
      "abstract": "",
      "date": "2025-09-08",
      "year": 2025,
      "venue": "OpenAlex",
      "status": "published",
      "categories": [
        {
          "label": "Others",
          "score": 0.0
        }
      ],
      "keywords": [],
      "curated": false,
      "notes": null,
      "links": {
        "openalex": "https://openalex.org/W4414077364"
      },
      "sources": [
        {
          "source": "openalex",
          "seen_at": "2025-10-01T10:35:49Z"
        }
      ],
      "history": [],
      "last_updated": "2025-10-01T10:35:49Z"
    }
  },
  "stats": {
    "per_year": {
      "2025": 12
    },
    "per_category": {
      "Tools & Libraries": 6,
      "Novel Applications": 6,
      "Reinforcement Learning & Autonomous Systems": 2,
      "Surrogate Models": 2,
      "Statistics & Trends": 1,
      "Others": 2,
      "Data Management": 4
    },
    "per_venue/journal": {
      "arXiv": 9,
      "InspireHEP": 2,
      "OpenAlex": 1
    },
    "per_keyword": {
      "optimization": 1,
      "control": 2,
      "surrogate model": 2,
      "transformer": 3,
      "reinforcement learning": 1,
      "deep learning": 1
    },
    "monthly_trends": {
      "2025-09": 12
    }
  }
}